train.py

#Import requirements

!pip install chardet -q
import chardet
import pandas as pd

from imblearn.pipeline import Pipeline as ImbPipeline
from imblearn.over_sampling import SMOTE
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.model_selection import train_test_split

import matplotlib.pyplot as plt
import seaborn as sns

with open("/content/customer_booking.csv", "rb") as f:
    encoding = chardet.detect(f.read())["encoding"]

data = pd.read_csv("/content/customer_booking.csv", encoding=encoding)

#EDA
data.head()

# @title Average Length of Stay by Sales Channel
import matplotlib.pyplot as plt

# Group the data by sales channel and calculate the average length of stay
stay_by_channel = data.groupby('sales_channel')['length_of_stay'].mean()

# Create a bar chart
plt.bar(stay_by_channel.index, stay_by_channel.values)
plt.xlabel('Sales Channel')
plt.ylabel('Average Length of Stay')
_ = plt.title('Average Length of Stay by Sales Channel')

        
feature.py
# @title Purchase Lead vs Flight Day

import matplotlib.pyplot as plt
data.boxplot('purchase_lead', by='flight_day')
plt.xlabel('Flight Day')
_ = plt.ylabel('Purchase Lead')

# @title trip_type vs num_passengers

from matplotlib import pyplot as plt
import seaborn as sns
figsize = (12, 1.2 * len(data['trip_type'].unique()))
plt.figure(figsize=figsize)
sns.violinplot(data, x='num_passengers', y='trip_type', inner='box',  palette='Dark2')
sns.despine(top=True, right=True, bottom=True, left=True)

# @title purchase_lead

from matplotlib import pyplot as plt
data['purchase_lead'].plot(kind='hist', bins=20, title='purchase_lead')
plt.gca().spines[['top', 'right',]].set_visible(False)

# @title num_passengers

from matplotlib import pyplot as plt
data['num_passengers'].plot(kind='hist', bins=20, title='num_passengers')
plt.gca().spines[['top', 'right',]].set_visible(False)

# @title sales_channel vs purchase_lead

from matplotlib import pyplot as plt
import seaborn as sns
figsize = (12, 1.2 * len(data['sales_channel'].unique()))
plt.figure(figsize=figsize)
sns.violinplot(data, x='purchase_lead', y='sales_channel', inner='box',  palette='Dark2')
sns.despine(top=True, right=True, bottom=True, left=True)


data.describe()
data.columns
data.info()

# Check correlation among numerical features
correlation_matrix = data.corr()
plt.figure(figsize=(10, 8))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=".2f")
plt.title('Correlation Matrix')
plt.show()



# Separate features and target
X = data.drop('booking_complete', axis=1)
y = data['booking_complete']

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)


# Identify numerical and categorical columns
numerical_cols = X.select_dtypes(include=['int64', 'float64']).columns.tolist()
categorical_cols = X.select_dtypes(include=['object']).columns.tolist()

# Create transformers for numerical and categorical features
numerical_transformer = StandardScaler()
categorical_transformer = OneHotEncoder()

# Create a column transformer
preprocessor = ColumnTransformer(
    transformers=[
        ('num', numerical_transformer, numerical_cols),
        ('cat', categorical_transformer, categorical_cols)
    ],
    remainder='passthrough'
)


# Create a pipeline with preprocessing and SMOTE steps
pipeline = ImbPipeline([
    ('preprocessor', preprocessor),
    ('smote', SMOTE(random_state=42)),
])

# Fit and transform the training data
X_train_resampled, y_train_resampled = pipeline.fit_resample(X_train, y_train)


class_counts = data["booking_complete"].value_counts()

print("Class Distribution (Before SMOTE):")
for label, count in class_counts.items():
    print(f"Class {label}: {count} samples")


# Print the count of each class in the final preprocessed data
class_counts = y_train_resampled.value_counts()




print("Class Distribution (After SMOTE):")
for label, count in class_counts.items():
print(f"Class {label}: {count} samples")
        
		
test.py
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score
import chardet

# # File has encoding issues, using chardet we have decoded the error.
# with open("/content/customer_booking.csv", "rb") as f:
#     encoding = chardet.detect(f.read())["encoding"]

# data = pd.read_csv("/content/customer_booking.csv", encoding=encoding)

# Separate features and target
X = data.drop('booking_complete', axis=1)
y = data['booking_complete']

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Identify numerical and categorical columns
numerical_cols = X.select_dtypes(include=['int64', 'float64']).columns.tolist()
categorical_cols = X.select_dtypes(include=['object']).columns.tolist()

# Create transformers for numerical and categorical features
numerical_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='mean')),
    ('scaler', StandardScaler())
])

categorical_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),
    ('onehot', OneHotEncoder(handle_unknown='ignore'))
])

# Create a column transformer
preprocessor = ColumnTransformer(
    transformers=[
        ('num', numerical_transformer, numerical_cols),
        ('cat', categorical_transformer, categorical_cols)
    ],
    remainder='drop'
)

# Define classifiers
classifiers = {
    'RandomForest': RandomForestClassifier(),
    'GradientBoosting': GradientBoostingClassifier(),
    'AdaBoost': AdaBoostClassifier(),
    'SVM': SVC()
}


# Define evaluation metrics
metrics = {
    'Accuracy': accuracy_score,
    'Precision': precision_score,
    'Recall': recall_score,
    'F1 Score': f1_score,
    'ROC AUC': roc_auc_score
}

# Define a function to evaluate models
def evaluate_model(model, X_test, y_test):
    y_pred = model.predict(X_test)
    results = {}
    for metric_name, metric_func in metrics.items():
        results[metric_name] = metric_func(y_test, y_pred)
    return results

# Train and evaluate models
model_results = {}
for model_name, classifier in classifiers.items():
    # Create a pipeline with preprocessor and classifier
    model_pipeline = Pipeline(steps=[
        ('preprocessor', preprocessor),
        ('classifier', classifier)
    ])

    # Fit the model on the training data
    model_pipeline.fit(X_train, y_train)

    # Evaluate the model
    model_results[model_name] = evaluate_model(model_pipeline, X_test, y_test)


# Print model evaluation results
for model_name, results in model_results.items():
    print(f"{model_name} Metrics:")
    for metric_name, metric_value in results.items():
        print(f"{metric_name}: {metric_value}")
    print("\n")


import numpy as np
from sklearn.model_selection import GridSearchCV, train_test_split
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout
from sklearn.metrics import make_scorer, accuracy_score
from sklearn.base import BaseEstimator
from sklearn.datasets import make_classification

# Generate synthetic binary classification data
X, y = make_classification(n_samples=1000, n_features=20, n_classes=2, random_state=42)

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Define a custom KerasClassifier class
class CustomKerasClassifier(BaseEstimator):
    def __init__(self, neurons=64, dropout_rate=0.2, batch_size=32, epochs=10):
        self.neurons = neurons
        self.dropout_rate = dropout_rate
        self.batch_size = batch_size
        self.epochs = epochs

    def create_model(self, input_shape):
        model = Sequential([
            Dense(self.neurons, activation='relu', input_shape=(input_shape,)),
            Dropout(self.dropout_rate),
            Dense(self.neurons // 2, activation='relu'),
            Dropout(self.dropout_rate),
            Dense(self.neurons // 4, activation='relu'),
            Dropout(self.dropout_rate),
            Dense(1, activation='sigmoid')  # Binary classification requires a sigmoid activation function
        ])
        model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
        return model

    def fit(self, X, y):
        input_shape = X.shape[1]
        self.model = self.create_model(input_shape)
        self.model.fit(X, y, epochs=self.epochs, batch_size=self.batch_size, verbose=0)

    def predict(self, X):
        return (self.model.predict(X) > 0.5).astype(int)  # Thresholding predictions for binary classification

# Define the parameter grid for grid search
param_grid = {
    'neurons': [64, 128, 256],
    'dropout_rate': [0.1, 0.2, 0.3],
    'batch_size': [32, 64],
    'epochs': [10, 20]
}

# Define custom scoring function for accuracy
def accuracy_custom(y_true, y_pred):
    return accuracy_score(y_true, y_pred)

accuracy_scorer = make_scorer(accuracy_custom)

# Create an instance of CustomKerasClassifier
keras_classifier = CustomKerasClassifier()

# Perform grid search
grid_search = GridSearchCV(estimator=keras_classifier, param_grid=param_grid, cv=3, scoring=accuracy_scorer)
grid_result = grid_search.fit(X_train, y_train)

# Print the best hyperparameters found
print("Best Accuracy: %f using %s" % (grid_result.best_score_, grid_result.best_params_))

# Make predictions with the best model
y_pred = grid_result.predict(X_test)

# Calculate evaluation metrics
accuracy = accuracy_score(y_test, y_pred)

print(f'Accuracy: {accuracy}')

# Print the best hyperparameters found
print("Best Accuracy: %f using %s" % (grid_result.best_score_, grid_result.best_params_))

# Extract and print the best hyperparameters
best_neurons = grid_result.best_params_['neurons']
best_dropout_rate = grid_result.best_params_['dropout_rate']
best_batch_size = grid_result.best_params_['batch_size']
best_epochs = grid_result.best_params_['epochs']

print(f"Best FNN Hyperparameters:")
print(f"Neurons: {best_neurons}")
print(f"Dropout Rate: {best_dropout_rate}")
print(f"Batch Size: {best_batch_size}")
print(f"Epochs: {best_epochs}")

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix

# Calculate evaluation metrics
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)
conf_matrix = confusion_matrix(y_test, y_pred)

print(f'Accuracy: {accuracy}')
print(f'Precision: {precision}')
print(f'Recall: {recall}')
print(f'F1 Score: {f1}')
print('Confusion Matrix:')
print(conf_matrix)


